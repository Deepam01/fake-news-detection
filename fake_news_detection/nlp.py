# -*- coding: utf-8 -*-
"""NLP_Improved.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SGaTW8tF9zHFpKQIb5kmCr5MEOsdmRmT
"""

import pandas as pd
import re
import nltk
import joblib

from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Download required NLTK data (only first time)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

# -------------------------
# Text Preprocessing Function
# -------------------------
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'\[.*?\]', '', text)                     # Remove [text]
    text = re.sub(r'https?://\S+|www\.\S+', '', text)       # Remove URLs
    text = re.sub(r'<.*?>+', '', text)                      # Remove HTML tags
    text = re.sub(r'[^a-z\s]', '', text)                    # Keep only letters
    text = re.sub(r'\d+', '', text)                         # Remove numbers
    tokens = nltk.word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    filtered = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return ' '.join(filtered)

# -------------------------
# Load and Label Dataset
# -------------------------
def load_data():
    try:
        fake_news = pd.read_csv('/content/Fake.csv')
        true_news = pd.read_csv('/content/True.csv')
    except FileNotFoundError:
        print("‚ö†Ô∏è CSV files not found. Using dummy dataset.")
        fake_news = pd.DataFrame({'text': ["Fake article here.", "Totally fabricated info."]})
        true_news = pd.DataFrame({'text': ["Real news update.", "Verified and authentic content."]})

    fake_news['label'] = 0
    true_news['label'] = 1
    return pd.concat([fake_news[['text', 'label']], true_news[['text', 'label']]], ignore_index=True)

# -------------------------
# Train and Evaluate Model
# -------------------------
def train_model(df):
    df['text'] = df['text'].apply(preprocess_text)

    X = df['text']
    y = df['label']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    vectorizer = TfidfVectorizer(max_features=5000)
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)

    model = LogisticRegression(max_iter=1000)
    model.fit(X_train_vec, y_train)

    predictions = model.predict(X_test_vec)
    accuracy = accuracy_score(y_test, predictions)

    print(f"\n‚úÖ Model Accuracy: {accuracy:.4f}\n")
    print("üîç Classification Report:\n", classification_report(y_test, predictions, target_names=["Fake", "Real"]))

    # Save model and vectorizer
    joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')
    joblib.dump(model, 'fake_news_model.pkl')
    print("\nüíæ Model and vectorizer saved successfully.")

# -------------------------
# Main Execution
# -------------------------
if __name__ == "__main__":
    df = load_data()
    train_model(df)